{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run 'data.py'\n",
    "%run 'distances.py'\n",
    "%run 'hott.py'\n",
    "%run 'knn_classifier.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On load le dataframe sur lequel on travaille (ici on prend un tout petit subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/Hugo/Documents/Cours/ENSAE/3A/S2/NLP/Projet/data/nyt-comments'\n",
    "embed_path_50 = '/Users/Hugo/Documents/Cours/ENSAE/3A/S2/NLP/Projet/data/glove.6B/glove.6B.300d.txt'\n",
    "df_ = pd.read_csv(glob.glob(data_dir+'/Comments*.csv')[0], encoding = 'latin1')\n",
    "df_ = df_.sample(frac=1).reset_index(drop=True)\n",
    "#cols = df_.columns\n",
    "df_test = df_[0:1000].reset_index(drop = True)\n",
    "#del df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Gibbs topics\n",
      "Topic 0: as me to a i not miss ryan will partisan be anyone beyond this possible seems scenario him how nightmare\n",
      "Topic 1: with no work some has potential it as and mechanism identified been ongoing cause a study causal trump wow egypt\n",
      "Topic 2: the of a is to and that in be as for or his with i not have would from we\n",
      "Topic 3: in the rudy was and on investigations hrc york something when reopening got comey by smirked thereafter made tipped fbi\n",
      "Topic 4: the is trump why you a to in so and that of was an for if his bad he who\n",
      "Topic 5: exactly sincerely fresh collaboration credibility defending machine lie delivered tricks audience builtin peddled yells various seth dirty describing confirmed hollein\n",
      "Topic 6: the of a you and in to would was your on what is as they about at not for many\n",
      "Topic 7: the of a to and is that in for are it this they as be his have not he or\n",
      "Topic 8: the and to in a they you that we as can with are all will be of or children more\n",
      "Topic 9: be i will thin is maybe he dysfunctional horrible not things do by that fabulous any artists original from means\n",
      "Topic 10: and a i for about one it an the has my of she was in become just this never more\n",
      "Topic 11: the of to in and is all it that a war has an on act as from lie are says\n",
      "Topic 12: the that to and with a trump is his mr does was way for of no just he not it\n",
      "Topic 13: the is it you to a fox all when as i and they on for be not advertisers majority your\n",
      "Topic 14: and the to a that is of this in they show from as for dont why but their like you\n",
      "Topic 15: the to not and of a in that for or is with do those are may we they will by\n",
      "Topic 16: the a he of for was and you in all on be henry women that will to about are did\n",
      "Topic 17: the a will that is have for our by to soil book are or nra harry potter be big an\n",
      "Topic 18: the and of to is in for a this be that as people their they all with or it his\n",
      "Topic 19: and to in a be of the we our not religious education on for might beliefs discourse school your or\n",
      "Topic 20: the and of to in a that is i for it have not as their on be was this they\n",
      "Topic 21: the a is he and to for that you in has but it trump i this be border rockaway potus\n",
      "Topic 22: a the i to and of that for not is in trump he they but this people any be from\n",
      "Topic 23: the and to that of i a was is not for in he with as it have has be on\n",
      "Topic 24: the is to a of for that i it not in have this and are think well should all be\n",
      "Topic 25: the to and of in a that their they not for have is who it on park public our with\n",
      "Topic 26: the and of a to is in that are they you like on have it for trump i not his\n",
      "Topic 27: the and to of a in china that it is for was from would are all you as on an\n",
      "Topic 28: the to is and it of a in not that can be if we this by as are isreal must\n",
      "Topic 29: the a to of you was is that reporter and in what had on it work for black times this\n",
      "Topic 30: the to is and of a in this has will it not on he that trump with an who or\n",
      "Topic 31: the a is and that with of to are or on for he it in will i people level republican\n",
      "Topic 32: the it is if trump with from and was a tax top on to get i yes could do they\n",
      "Topic 33: the to a of and in is our that have it this not at we for trump as are they\n",
      "Topic 34: and to the of it is i a in that my he what their have are they government be us\n",
      "Topic 35: the to a we and that of is but it need be are they or in want must dont so\n",
      "Topic 36: i it my to a me for like the have time you they and not that would or this look\n",
      "Topic 37: the to and in a of that is for this he are us we by as all with or his\n",
      "Topic 38: the to a is and of in that this not be it with by you their are your for who\n",
      "Topic 39: the of in to that and a is be like with will for was not i this they their what\n",
      "Topic 40: of the to and for a in it not this that who he they his many their with i deal\n",
      "Topic 41: to for your be the you and or one of this that it think mr own but we a is\n",
      "Topic 42: the of she is trump to your and in her not hard at that it as for this least on\n",
      "Topic 43: the to a and of is i we have want would with for was been us no in she did\n",
      "Topic 44: the to and is of in a for he it will not this as that can i we trump its\n",
      "Topic 45: the to in for a of her she and be that you are not have is or they as it\n",
      "Topic 46: the to a of and in i for his trump was on not they be this an is there that\n",
      "Topic 47: female fashion males to designers jeans design designing the very because a are which agenda mitch seat nothing stolen theres\n",
      "Topic 48: and the what was is that in a to of at on this it darkness so federal just with or\n",
      "Topic 49: the to people is were this in are affixed have who 2000 stopped blinders that a over border firmly these\n",
      "Topic 50: amazing clothes in emperors spades the various yells peddled seth dirty builtin audience scalise delivered fresh lie machine defending credibility\n",
      "Topic 51: the be that for to is leonhardt like an of mr as they i big when will from a are\n",
      "Topic 52: the to a and of is that i in with for are it they not or do have but who\n",
      "Topic 53: the to of and is he his it a in trump for was us man not by be trumps has\n",
      "Topic 54: to a and so or you more just people smile at happy could i person make why we the our\n",
      "Topic 55: the to and of in a an with not but be just if their i is would one who thats\n",
      "Topic 56: the for of is you feeding an to and not this same treatment oral medical if no tapering off arrest\n",
      "Topic 57: to the of and a is for be they are that in it what has people this who on credit\n",
      "Topic 58: and the to a is of in that are you for not it they have i on can more this\n",
      "Topic 59: the to i of and is a his trump with for would it who but like that some or was\n",
      "Topic 60: it if of only see you confirmed confirms sure an view have confirmation a what like the extension lie bias\n",
      "Topic 61: the to a in was and it of are that or us is what i horse from social people white\n",
      "Topic 62: to are and of is for that this white those with you analysis by have his people the in around\n",
      "Topic 63: the a to and it is we of do as in like you all want work have saved building homelessness\n",
      "Topic 64: the i you that to in a and was of russia is people this him not but his be at\n",
      "Topic 65: apparently bureaucrat typical a or public official peddled builtin audience delivered fresh describing lie machine defending credibility collaboration dirty tricks\n",
      "Topic 66: the and of is or about very no on its even person they a bad americans think innuendo so in\n",
      "Topic 67: the to are of and is this trump a that would they for in it have i not our as\n",
      "Topic 68: the to and of a is in that for are as be with i on not this his he it\n",
      "Topic 69: the to and a for in of i it was is this that be its my not than on more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_data(df_test, embed_path_50, stemming = False, K = 70, p = 1, n_word_keep = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revoir la fonction qui enl√®ve les stops words car elle ne semble pas fonctionner : pourrait expliquer les mauvaises perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train, bow_test, topic_train, topic_test, y_train, y_test = train_test_split(data['X'],\n",
    "                                                                                 data['proportions'],\n",
    "                                                                                 data['y'],\n",
    "                                                                                 random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = data['cost_T']\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour un un nombre de voisin 7 l'erreur sur la base de test est 0.748\n",
      "Pour un un nombre de voisin 9 l'erreur sur la base de test est 0.776\n",
      "Pour un un nombre de voisin 11 l'erreur sur la base de test est 0.78\n",
      "Pour un un nombre de voisin 13 l'erreur sur la base de test est 0.772\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6d13e7e42c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We could test with wmd and other distances in the file distances.py, however, the computational time may limit us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneigh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhott_test_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhott_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhott\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pour un un nombre de voisin %s l\\'erreur sur la base de test est %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhott_test_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/ENSAE/3A/S2/NLP/Projet/knn_classifier.py\u001b[0m in \u001b[0;36mknn\u001b[0;34m(X_train, X_test, y_train, y_test, method, C, n_neighbors)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Make prediction based on most popular class among neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Print and return test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/ENSAE/3A/S2/NLP/Projet/knn_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(neighbor_classes, C)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# If most popular class is ambiguous try with fewer neighbors; else return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbor_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m~/Documents/Cours/ENSAE/3A/S2/NLP/Projet/knn_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(neighbor_classes, C)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# If most popular class is ambiguous try with fewer neighbors; else return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbor_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "# We could test with wmd and other distances in the file distances.py, however, the computational time may limit us\n",
    "for neigh in :\n",
    "    hott_test_error, hott_pred = knn(topic_train, topic_test, y_train, y_test, hott, C, n_neighbors=neigh)\n",
    "    print('Pour un un nombre de voisin %s l\\'erreur sur la base de test est %s'%(neigh,hott_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
